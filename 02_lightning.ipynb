{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from test_tube import HyperOptArgumentParser\n",
    "from emotion_transformer.dataloader import dataloader\n",
    "from emotion_transformer.model import sentence_embeds_model, context_classifier_model, metrics, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning module for the SemEval Challenge\n",
    "\n",
    "> construction of the PyTorch Lightning module and the hyperparameter search for the SemEval-2019 Task 3 dataset (contextual emotion detection in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Module\n",
    "\n",
    "Defining the Lightning module is now straightforward, see also the [documentation](https://williamfalcon.github.io/pytorch-lightning/). The default hyperparameter choices were motivated by [this paper](https://arxiv.org/pdf/1905.05583.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EmotionModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for the Contextual Emotion Detection in Text Challenge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"\n",
    "        pass in parsed HyperOptArgumentParser to the model\n",
    "        \"\"\"\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.emo_dict = {'others': 0, 'sad': 1, 'angry': 2, 'happy': 3}\n",
    "        self.sentence_embeds_model = sentence_embeds_model(hparams.projection_size,\n",
    "                                                           dropout = hparams.dropout)\n",
    "        self.context_classifier_model = context_classifier_model(hparams.projection_size, \n",
    "                                                                 hparams.n_layers, \n",
    "                                                                 self.emo_dict, \n",
    "                                                                 dropout = hparams.dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels = None):\n",
    "        \"\"\"\n",
    "        no special modification required for lightning, define as you normally would\n",
    "        \"\"\"\n",
    "        \n",
    "        sentence_embeds = self.sentence_embeds_model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        return self.context_classifier_model(sentence_embeds = sentence_embeds, labels = labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        \"\"\"       \n",
    "        input_ids, attention_mask, labels = batch\n",
    "        loss, _ = self.forward(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the validation loop\n",
    "        \"\"\"\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        loss, logits = self.forward(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "        scores_dict = metrics(loss, logits, labels)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            scores = [score.unsqueeze(0) for score in scores_dict.values]\n",
    "            scores_dict = {key: value for key, value in zip(scores_dict.keys, scores)}\n",
    "\n",
    "        return scores_dict\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        \"\"\"\n",
    "        called at the end of validation to aggregate outputs\n",
    "        :param outputs: list of individual outputs of each validation step\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        tqdm_dict = {}\n",
    "\n",
    "        for metric_name in outputs[0].keys():\n",
    "            metric_total = 0\n",
    "\n",
    "            for output in outputs:\n",
    "                metric_value = output[metric_name]\n",
    "\n",
    "                if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                    print(metric_value.shape)\n",
    "                    if metric_name in ['tp', 'fp', 'fn']:\n",
    "                        metric_value = torch.sum(metric_value)\n",
    "                    else:\n",
    "                        metric_value = torch.mean(metric_value)\n",
    "                    \n",
    "                metric_total += metric_value\n",
    "            if metric_name in ['tp', 'fp', 'fn']:\n",
    "                tqdm_dict[metric_name] = metric_total\n",
    "            else:\n",
    "                tqdm_dict[metric_name] = metric_total / len(outputs)\n",
    "\n",
    "               \n",
    "        prec_rec_f1 = f1_score(tqdm_dict['tp'], tqdm_dict['fp'], tqdm_dict['fn'])\n",
    "        tqdm_dict.update(prec_rec_f1) \n",
    "        result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': tqdm_dict[\"val_loss\"]}\n",
    "        return result\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        returns the optimizer and scheduler\n",
    "        \"\"\"\n",
    "        opt_parameters = self.sentence_embeds_model.layerwise_lr(self.hparams.lr, \n",
    "                                                                 self.hparams.layerwise_decay)\n",
    "        opt_parameters += [{'params': self.context_classifier_model.parameters()}]\n",
    "\n",
    "        optimizer = torch.optim.Adam(opt_parameters, lr=self.hparams.lr)        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return dataloader(self.hparams.train_file, self.hparams.max_seq_len, \n",
    "                          self.hparams.bs, self.emo_dict, use_ddp = self.use_ddp)\n",
    "\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return dataloader(self.hparams.val_file, self.hparams.max_seq_len, \n",
    "                          self.hparams.bs, self.emo_dict, use_ddp = self.use_ddp)\n",
    "\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        return dataloader(self.hparams.test_file, self.hparams.max_seq_len, \n",
    "                          self.hparams.bs, self.emo_dict, use_ddp = self.use_ddp)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser, root_dir):  \n",
    "        \"\"\"\n",
    "        parameters defined here will be available to the model through self.hparams\n",
    "        \"\"\"\n",
    "        parser = HyperOptArgumentParser(parents=[parent_parser])\n",
    "\n",
    "        parser.opt_list('--bs', '--batch_size', default=40, type=int, \n",
    "                        options=[10, 40, 80], tunable=True, metavar='N',\n",
    "                        help='mini-batch size (default: 256), this is the'\n",
    "                        'total batch size of all GPUs on the current node'\n",
    "                        'when using Data Parallel or Distributed Data Parallel')\n",
    "        parser.opt_list('--max_seq_len', default=10, type=int, options=[5, 10, 15], tunable=True)\n",
    "        parser.opt_list('--projection_size', default=100, type=int, options=[50, 100, 150], tunable=True)\n",
    "\n",
    "        parser.opt_list('--dropout', default=0.1, type=float, options=[0.05, 0.1, 0.2], tunable=True)\n",
    "        parser.opt_list('--n_layers', default=1, type=int, options=[1, 2, 3], tunable=True)\n",
    "        parser.opt_range('--lr', '--learning_rate', default=2.0e-5, type=float, \n",
    "                         tunable=True, low=1.0e-5, high=3.0e-5, nb_samples=4,\n",
    "                         help='initial learning rate', metavar='LR', dest='lr')\n",
    "        parser.opt_list('--layerwise_decay', default=0.95, type=float, options=[0.99, 0.95, 0.9], tunable=True)\n",
    "        parser.add_argument('--train_file', default=os.path.join(root_dir, 'data/clean_train.txt'), type=str)\n",
    "        parser.add_argument('--val_file', default=os.path.join(root_dir, 'data/clean_val.txt'), type=str)\n",
    "        parser.add_argument('--test_file', default=os.path.join(root_dir, 'data/clean_test.txt'), type=str)\n",
    "        parser.add_argument('--epochs', default=10, type=int, metavar='N',\n",
    "                            help='number of total epochs to run')\n",
    "        parser.add_argument('--seed', type=int, default=None,\n",
    "                            help='seed for initializing training. ')\n",
    "        \n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the HyperOptArgumentParser including distributed training (see also the [documentation](https://williamfalcon.github.io/pytorch-lightning/Trainer/Distributed%20training/\n",
    ")) and debugging functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_args(model):\n",
    "    \"\"\"\n",
    "    returns the HyperOptArgumentParser\n",
    "    \"\"\"\n",
    "    parent_parser = HyperOptArgumentParser(strategy='random_search',\n",
    "                                           add_help = False)\n",
    "\n",
    "    root_dir = os.getcwd()  \n",
    "    parent_parser.add_argument('--mode', type=str, default='default', choices=('default', 'hparams_search'),\n",
    "                               help='supports default for train/test/val and hparams_search for a hyperparameter search')\n",
    "    parent_parser.add_argument('--save-path', metavar='DIR', default=os.path.join(root_dir, 'logs'), type=str,\n",
    "                               help='path to save output')\n",
    "    parent_parser.add_argument('--gpus', type=int, default=0,\n",
    "                               help='how many gpus')\n",
    "    parent_parser.add_argument('--distributed-backend', type=str, default='dp', choices=('dp', 'ddp', 'ddp2'),\n",
    "                               help='supports three options dp, ddp, ddp2')\n",
    "    parent_parser.add_argument('--use_16bit', dest='use_16bit', action='store_true',\n",
    "                               help='if true uses 16 bit precision')\n",
    "\n",
    "    # debugging\n",
    "    parent_parser.add_argument('--fast_dev_run', dest='fast_dev_run', action='store_true',\n",
    "                               help='debugging a full train/val/test loop')\n",
    "    parent_parser.add_argument('--track_grad_norm', dest='track_grad_norm', action='store_true',\n",
    "                               help='inspect gradient norms')\n",
    "    parent_parser.add_argument('--overfit_on_subset', default=0.0, type=float,\n",
    "                                help='debugging trick to make model overfit the specified fraction of the data')\n",
    "\n",
    "    parser = model.add_model_specific_args(parent_parser, root_dir) \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the different attributes of `hparams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'default',\n",
       " 'save_path': '/home/julius/Documents/nbdev_venv/emotion_transformer/logs',\n",
       " 'gpus': 0,\n",
       " 'distributed_backend': 'dp',\n",
       " 'use_16bit': False,\n",
       " 'fast_dev_run': False,\n",
       " 'track_grad_norm': False,\n",
       " 'overfit_on_subset': 0.0,\n",
       " 'bs': 40,\n",
       " 'max_seq_len': 10,\n",
       " 'projection_size': 100,\n",
       " 'dropout': 0.1,\n",
       " 'n_layers': 1,\n",
       " 'lr': 2e-05,\n",
       " 'layerwise_decay': 0.95,\n",
       " 'train_file': '/home/julius/Documents/nbdev_venv/emotion_transformer/data/clean_train.txt',\n",
       " 'val_file': '/home/julius/Documents/nbdev_venv/emotion_transformer/data/clean_val.txt',\n",
       " 'test_file': '/home/julius/Documents/nbdev_venv/emotion_transformer/data/clean_test.txt',\n",
       " 'epochs': 10,\n",
       " 'seed': None,\n",
       " 'hpc_exp_number': None,\n",
       " 'trials': <bound method HyperOptArgumentParser.opt_trials of HyperOptArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)>,\n",
       " 'optimize_parallel': <bound method HyperOptArgumentParser.optimize_parallel of HyperOptArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)>,\n",
       " 'optimize_parallel_gpu': <bound method HyperOptArgumentParser.optimize_parallel_gpu of HyperOptArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)>,\n",
       " 'optimize_parallel_cpu': <bound method HyperOptArgumentParser.optimize_parallel_cpu of HyperOptArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)>,\n",
       " 'generate_trials': <bound method HyperOptArgumentParser.generate_trials of HyperOptArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)>,\n",
       " 'optimize_trials_parallel_gpu': <bound method HyperOptArgumentParser.optimize_trials_parallel_gpu of HyperOptArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = get_args(EmotionModel)\n",
    "hparams = hparams.parse_args(args=[])\n",
    "vars(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function calling the Lightning trainer and saving checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def main(hparams):\n",
    "    \"\"\"\n",
    "    Trains the Lightning model as specified in `hparams`\n",
    "    \"\"\"\n",
    "    model = EmotionModel(hparams)\n",
    "    \n",
    "    if hparams.seed is not None:\n",
    "        random.seed(hparams.seed)\n",
    "        torch.manual_seed(hparams.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(hparams.save_path, 'model_checkpoints'),\n",
    "        save_best_only=True,\n",
    "        verbose=True,\n",
    "        monitor='f1_score',\n",
    "        mode='max',\n",
    "        prefix=''\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(default_save_path=hparams.save_path,\n",
    "                        gpus=hparams.gpus,\n",
    "                        distributed_backend=hparams.distributed_backend,\n",
    "                        use_amp=hparams.use_16bit,\n",
    "                        checkpoint_callback=checkpoint_callback,\n",
    "                        max_nb_epochs=hparams.epochs,\n",
    "                        weights_summary='top',\n",
    "                        fast_dev_run=hparams.fast_dev_run,\n",
    "                        track_grad_norm=(2 if hparams.track_grad_norm else -1),\n",
    "                        overfit_pct=hparams.overfit_on_subset,\n",
    "                        )\n",
    "\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the model by running a quick development run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1/2 [00:03<00:03,  3.02s/batch, batch_nb=0, loss=2.090, v_nb=0]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:03<00:00,  2.36s/batch, batch_nb=0, f1_score=0.125, fn=5, fp=37, loss=2.090, precision=0.075, recall=0.375, tp=3, v_nb=0, val_acc=0.075, val_loss=2.08]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:05<00:00,  2.87s/batch, batch_nb=0, f1_score=0.125, fn=5, fp=37, loss=2.090, precision=0.075, recall=0.375, tp=3, v_nb=0, val_acc=0.075, val_loss=2.08]\n"
     ]
    }
   ],
   "source": [
    "hparams.fast_dev_run = True\n",
    "main(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a python file for automatic hyperparameter optimization across different gpus or cpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "from emotion_detect.core import EmotionModel, get_args, main\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hparams = get_args(EmotionModel)\n",
    "    hparams = hparams.parse_args()\n",
    "\n",
    "    if hparams.mode == 'default':\n",
    "        main(hparams)\n",
    "    elif hparams.mode == 'hparams_search':\n",
    "        if hparams.gpus == 0:\n",
    "            hparams.optimize_parallel_cpu(main, nb_trials=20, nb_workers=1)\n",
    "        else:\n",
    "            hparams.optimize_parallel_gpu(main, nb_trials=20, gpus = list(range(hparams.gpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataloader.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted 02_lightning.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
